{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vladislav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Vladislav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\Vladislav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.38764778542057643\n",
      "Epoch 1, Validation Loss: 0.2262211542665249\n",
      "Epoch 1, Validation Accuracy: 0.932\n",
      "Epoch 2, Train Loss: 0.23686790784365302\n",
      "Epoch 2, Validation Loss: 0.23470048251606168\n",
      "Epoch 2, Validation Accuracy: 0.921\n",
      "Epoch 3, Train Loss: 0.16323970554718117\n",
      "Epoch 3, Validation Loss: 0.24755202553841094\n",
      "Epoch 3, Validation Accuracy: 0.906\n",
      "Epoch 4, Train Loss: 0.11652130150929656\n",
      "Epoch 4, Validation Loss: 0.292826375894485\n",
      "Epoch 4, Validation Accuracy: 0.915\n",
      "Epoch 5, Train Loss: 0.08780664074043709\n",
      "Epoch 5, Validation Loss: 0.27222338402163354\n",
      "Epoch 5, Validation Accuracy: 0.932\n",
      "Epoch 6, Train Loss: 0.07137592402069072\n",
      "Epoch 6, Validation Loss: 0.31697332576316384\n",
      "Epoch 6, Validation Accuracy: 0.936\n",
      "Epoch 7, Train Loss: 0.06094767963438355\n",
      "Epoch 7, Validation Loss: 0.28514823123502236\n",
      "Epoch 7, Validation Accuracy: 0.921\n",
      "Test Accuracy: 0.886\n"
     ]
    }
   ],
   "source": [
    "# 1. Подключение библиотек\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 2. Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "valid_df = pd.read_csv('validation.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Преобразуем столбцы в массивы\n",
    "X_train = train_df['dialog'].values\n",
    "y_train = train_df['emotion'].values\n",
    "X_valid = valid_df['dialog'].values\n",
    "y_valid = valid_df['emotion'].values\n",
    "X_test = test_df['dialog'].values\n",
    "y_test = test_df['emotion'].values\n",
    "\n",
    "y_train = [str(val).split()[0] for val in y_train]  # Берем первое число из массива\n",
    "y_valid = [str(val).split()[0] for val in y_valid]  # Берем первое число из массива\n",
    "y_test = [str(val).split()[0] for val in y_test]  # Берем первое число из массива\n",
    "\n",
    "# Теперь применяем LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_valid = label_encoder.transform(y_valid)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "# 4. Создание Dataset для PyTorch\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, dialogs, labels, tokenizer, max_len):\n",
    "        self.dialogs = dialogs\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        dialog = self.dialogs[item]\n",
    "        label = self.labels[item]\n",
    "\n",
    "        # Токенизация\n",
    "        encoding = self.tokenizer(dialog, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "\n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 5. Создание токенизатора\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 6. Создание DataLoader для обучения и тестирования\n",
    "max_len = 128  # Максимальная длина последовательности\n",
    "\n",
    "train_dataset = ChatDataset(X_train, y_train, tokenizer, max_len)\n",
    "valid_dataset = ChatDataset(X_valid, y_valid, tokenizer, max_len)\n",
    "test_dataset = ChatDataset(X_test, y_test, tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 7. Создание модели\n",
    "class ChatModel(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(ChatModel, self).__init__()\n",
    "        # Используем предобученную модель BERT\n",
    "        from transformers import BertModel\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = output.pooler_output\n",
    "        output = self.dropout(pooled_output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Количество классов (эмоций)\n",
    "n_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Создание модели\n",
    "model = ChatModel(n_classes)\n",
    "model = model.cuda() if torch.cuda.is_available() else model\n",
    "\n",
    "# 8. Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 9. Функция для тренировки модели\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, n_epochs=7):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].cuda() if torch.cuda.is_available() else batch['input_ids']\n",
    "            attention_mask = batch['attention_mask'].cuda() if torch.cuda.is_available() else batch['attention_mask']\n",
    "            labels = batch['label'].cuda() if torch.cuda.is_available() else batch['label']\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Выводим информацию о потере на каждой эпохе\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {total_train_loss / len(train_loader)}')\n",
    "\n",
    "        # Тестируем модель на валидации\n",
    "        model.eval()\n",
    "        total_valid_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                input_ids = batch['input_ids'].cuda() if torch.cuda.is_available() else batch['input_ids']\n",
    "                attention_mask = batch['attention_mask'].cuda() if torch.cuda.is_available() else batch['attention_mask']\n",
    "                labels = batch['label'].cuda() if torch.cuda.is_available() else batch['label']\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss: {total_valid_loss / len(valid_loader)}')\n",
    "        print(f'Epoch {epoch + 1}, Validation Accuracy: {correct / total}')\n",
    "\n",
    "# 10. Тренировка модели\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer, n_epochs=7)\n",
    "\n",
    "# 11. Тестирование модели\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].cuda() if torch.cuda.is_available() else batch['input_ids']\n",
    "            attention_mask = batch['attention_mask'].cuda() if torch.cuda.is_available() else batch['attention_mask']\n",
    "            labels = batch['label'].cuda() if torch.cuda.is_available() else batch['label']\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {correct / total}')\n",
    "\n",
    "# 12. Тестируем модель\n",
    "test_model(model, test_loader)\n",
    "\n",
    "# 13. Сохранение модели\n",
    "torch.save(model.state_dict(), 'chat_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
